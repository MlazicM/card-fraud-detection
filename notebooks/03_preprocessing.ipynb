{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b50ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (284807, 31)\n",
      "Final shape: (283726, 31)\n",
      "Number of duplicate rows removed: 1081\n",
      "Time column dropped and hour_of_day feature added.\n",
      "Amount column scaled using log transformation.\n",
      "Train set shape: (226980, 30), Test set shape: (56746, 30)\n",
      "After SMOTE, Train set shape: (453204, 30), Test set shape: (56746, 30)\n"
     ]
    }
   ],
   "source": [
    "#Deleting 1081 duplicate rows from the dataset. This is done to avoid data leakage and to ensure that the model is trained on unique data points.\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/raw/creditcard.csv\")\n",
    "initial_shape = df.shape\n",
    "df.drop_duplicates(inplace=True)\n",
    "final_shape = df.shape\n",
    "print(f\"Initial shape: {initial_shape}\")\n",
    "print(f\"Final shape: {final_shape}\")\n",
    "print(f\"Number of duplicate rows removed: {initial_shape[0] - final_shape[0]}\")\n",
    "#Deleteing the 'Time' column as it is not relevant for the analysis and can be considered as a noise feature. Adding hour_of_day feature which can be useful for the analysis as it can help to identify patterns in the data based on the time of the day when the transactions were made.\n",
    "df.drop(columns=['Time'], inplace=True)\n",
    "df['hour_of_day'] = (df.index // 3600) % 24\n",
    "print(f\"Time column dropped and hour_of_day feature added.\")\n",
    "#Scaling the 'Amount' column- log trasformation \n",
    "import numpy as np\n",
    "df['Amount'] = np.log1p(df['Amount'])\n",
    "print(f\"Amount column scaled using log transformation.\")\n",
    "#Train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "#SMOTE \n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(f\"After SMOTE, Train set shape: {X_train_smote.shape}, Test set shape: {X_test.shape}\")\n",
    "#Saving the preprocessed data to csv files for future use\n",
    "X_train_smote.to_csv('../data/processed/X_train_smote.csv', index=False)\n",
    "y_train_smote.to_csv('../data/processed/y_train_smote.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d2d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
